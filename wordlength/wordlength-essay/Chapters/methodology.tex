\section{Methodology}
\subsection{Sourcing texts}
For this research, I used the Pali canon from the Mahāsaṅgīti Tipiṭaka Buddhavasse 2500 from the Vipassana Research Institute (\url{https://tipitaka.org/}).

These texts, with the exception of the commentarial texts, are used by SuttaCentral.net and they have partly segmented these files and coded them in json. These json files do not have any of the coding that is necessary to display texts online but just the pure texts, which makes it ideal to work with. This format is the basis I used for my analysis.

Where these files did not exist, I have taken them from the SuttaCentral HTML and coded them into the same segmented json format. The same I have done for the commentarial texts directly from the VRI website in XML.

The github repository for these json files is here: (\url{https://github.com/BuddhaNexus/segmented-Pali})

\subsubsection{Variant readings}
The Mahāsaṅgīti Tipiṭaka as used by the VRI as well as SuttaCentral also list various variant readings. These were removed for the purpose of this analysis. After analysing part of the variant readings I concluded that these would not make a significant shift in the calculations. In some cases variant words were a bit longer, in other cases shorter.

\subsubsection{Abbreviations}
Many texts contain abbreviations in the form of "… pe …" or similar. These abbreviations were removed prior to calculations because they would give a much lower average word-length for texts with many abbreviations. Of course this goes from the premise that the text being substituted has an overall average that is roughly the same as the still remaining text. I feel this is reasonable assumption and above all, it would be undo-able to replace the abbreviations with the text they are to replace.

\subsection{Other materials used}
In some instances I had to look into the parallels distribution of files within a collection. For this I used the parallels json file that is used by SuttaCentral. Although this material is not complete i.e. these are human sourced parallels and not all parallels will be known, they still give an indication. Moreover, these parallels also list known parallels with other Buddhist canons in other languages.

One major drawback in using this parallels-list for statistical analysis is that it is not consistent in how it counts parallels. For instance text A can be parallel to text B and this can list as one parallel. But it is also possible that a more detailed listing is applied in that for instance text A, verse 1 is parallel to text B, verse 2, etc. This multiplies the number of parallels considerably. 

In analysing some collections, I also made use of a sankey graphs from BuddhaNexus.net. Although this is very detailed, it only lists matches within the same canon. Matches within the same canon have their value, but in matching between various languages we can determine if texts were already in existence before the split of the schools and therefore their relative age\footnote{{\em The Authenticity of the Early Buddhist Texts} by Bhikkhu Sujato and Bhikkhu Brahmali}. The matching between languages is in the pipeline but to date not yet available. 

Note that BuddhaNexus graphs show the total lengths of matches so that a long match between texts counts for more than just one line, as opposed to the parallels listed in SuttaCentral, where every match just counts as 1. Therefore, it also takes the quality of the match into account.

For the descriptions of various texts I have used the descriptions as given on SuttaCentral.net\footnote{texts written by Bhikkhu Sujato} or on Wikipedia.

\subsection{Method of analysis}
The segmented files were further imported into an ArangoDB database and queries made from there, using the Python programming language for retrieving meaningful data for charts.

All punctuation and other typography markers and numbers were removed so only the alphanumerical characters were used for calculating the average word-length.

The average word-length (AWL) is calculated as:

\[
AWL = \frac{C}{W} = \frac{\sum_{i} c_{i}}{\sum_{i} w_{i}}
\]

Whereby 'c' is the total number of characters in a file and 'w' the total number of words. 'C' is the total number of characters in a collection and 'W' the total number of words therein. 

Note however that a file usually comprises of a sutta or text but in some cases it represents a chapter of a larger whole or have been merged to make one larger file within the collection. For the analysis of collections this does not make any difference but this can have an effect on the analysis of individual files within the collection.\\

Next to analysis the AWL of collections, I have done an analysis of the spread of the AWL within the collection as well as an analysis of the weight of each file's AWL on the total collection AWL, depending on it's length.

For this the following calculations were used for the File Character Weight (FCW) and File Word Weight (FWW):

\[
FCW = \frac{c}{C}*100\%
\]

\[
FWW = \frac{w}{W}*100\%
\]
